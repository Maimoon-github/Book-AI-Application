{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe1cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List, Dict, Tuple, Optional, Any, Sequence\n",
    "\n",
    "# RAG and AI imports\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "SUPPORTED_MODELS = [\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"gemma2-9b-it\",\n",
    "    \"mixtral-8x7b-32768\"\n",
    "]\n",
    "\n",
    "class BookTeachingRAG:\n",
    "    def __init__(self):\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        self.collection = None\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.groq_model = None\n",
    "        self.app = None\n",
    "\n",
    "    def setup_groq_model(self, api_key, model_name=\"llama-3.3-70b-versatile\"):\n",
    "        self.groq_model = ChatGroq(\n",
    "            model=model_name,\n",
    "            api_key=api_key,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        self._initialize_workflow()\n",
    "\n",
    "    def _initialize_workflow(self):\n",
    "        teaching_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert AI teacher specializing in adaptive learning from books. Your teaching follows a structured pedagogical approach.\\n\\n## Teaching Philosophy & Pattern:\\n\\n### 🎯 **Learning Objective Identification**\\n- First understand what the student wants to learn\\n- Identify their current knowledge level\\n- Set clear, achievable learning goals\\n\\n### 📚 **Structured Teaching Pattern**\\nWhen teaching any concept, follow this pattern:\\n\\n1. **FOUNDATION** 🏗️\\n   - Start with core definitions and basic concepts\\n   - Provide clear, simple explanations\\n   - Use analogies from everyday life\\n\\n2. **CONTEXT** 🌍\\n   - Explain where this fits in the bigger picture\\n   - Connect to previously learned material\\n   - Show relevance and importance\\n\\n3. **EXAMPLES** 💡\\n   - Provide concrete, relatable examples\\n   - Use case studies from the book content\\n   - Show practical applications\\n\\n4. **ANALYSIS** 🔍\\n   - Break down complex ideas into components\\n   - Explain cause-and-effect relationships\\n   - Highlight patterns and principles\\n\\n5. **APPLICATION** 🚀\\n   - Suggest how to apply this knowledge\\n   - Provide practice scenarios\\n   - Connect to real-world situations\\n\\n6. **REINFORCEMENT** 🎯\\n   - Summarize key takeaways\\n   - Suggest follow-up questions for deeper learning\\n   - Recommend related topics to explore\\n\\n### 💬 **Communication Style**\\n- **Clarity**: Use simple, clear language\\n- **Engagement**: Ask thought-provoking questions\\n- **Encouragement**: Maintain positive, supportive tone\\n- **Adaptation**: Adjust complexity based on student responses\\n- **Source-Grounded**: Always reference the book content\\n\\n### 🔄 **Interactive Learning**\\n- Ask \\\"Do you understand?\\\" or \\\"What would you like to explore further?\\\"\\n- Encourage questions and clarification requests\\n- Provide multiple perspectives on complex topics\\n- Use Socratic method when appropriate\\n\\n### 📖 **Content Integration**\\n- Always ground explanations in the provided book context\\n- Reference specific chapters and page numbers\\n- Quote relevant passages when helpful\\n- Maintain fidelity to the author's intent\\n\\n### 🎓 **Assessment & Progress**\\n- Check understanding with gentle questioning\\n- Provide positive reinforcement for engagement\\n- Suggest next steps in the learning journey\\n- Identify knowledge gaps and address them\\n\\nRemember: You are not just answering questions - you are facilitating deep, meaningful learning experiences based on the book's content.\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ])\n",
    "\n",
    "        class TeachingState(TypedDict):\n",
    "            messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "            context: str\n",
    "            sources: list\n",
    "\n",
    "        workflow = StateGraph(state_schema=TeachingState)\n",
    "\n",
    "        def call_teaching_model(state: TeachingState):\n",
    "            context_message = f\"\\n--- BOOK CONTEXT ---\\n{state.get('context', '')}\\n--- END CONTEXT ---\\n\"\n",
    "            messages_with_context = list(state[\"messages\"])\n",
    "            if messages_with_context and isinstance(messages_with_context[-1], HumanMessage):\n",
    "                last_msg = messages_with_context[-1]\n",
    "                enhanced_content = f\"{context_message}\\nSTUDENT QUESTION: {last_msg.content}\"\n",
    "                messages_with_context[-1] = HumanMessage(content=enhanced_content)\n",
    "            prompt = teaching_prompt.invoke({\"messages\": messages_with_context})\n",
    "            response = self.groq_model.invoke(prompt)\n",
    "            return {\"messages\": [response]}\n",
    "\n",
    "        workflow.add_node(\"teaching_model\", call_teaching_model)\n",
    "        workflow.add_edge(START, \"teaching_model\")\n",
    "        memory = MemorySaver()\n",
    "        self.app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "    def index_book_content(self, book_chunks):\n",
    "        try:\n",
    "            self.collection = self.chroma_client.create_collection(\n",
    "                name=\"book_content\",\n",
    "                get_or_create=True\n",
    "            )\n",
    "        except:\n",
    "            self.collection = self.chroma_client.get_collection(name=\"book_content\")\n",
    "\n",
    "        rag_chunks = self.create_rag_chunks(book_chunks)\n",
    "        documents = []\n",
    "        embeddings = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        for i, chunk in enumerate(rag_chunks):\n",
    "            embedding = self.embedding_model.encode(chunk['text'])\n",
    "            documents.append(chunk['text'])\n",
    "            embeddings.append(embedding.tolist())\n",
    "            metadatas.append(chunk['metadata'])\n",
    "            ids.append(f\"chunk_{i}\")\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "    def create_rag_chunks(self, book_chunks):\n",
    "        rag_chunks = []\n",
    "        for chapter in book_chunks:\n",
    "            chapter_text = chapter['content']\n",
    "            sentences = chapter_text.split('. ')\n",
    "            current_chunk = \"\"\n",
    "            word_count = 0\n",
    "            for sentence in sentences:\n",
    "                sentence_words = len(sentence.split())\n",
    "                if word_count + sentence_words > 400:\n",
    "                    if current_chunk.strip():\n",
    "                        rag_chunks.append({\n",
    "                            'text': current_chunk.strip(),\n",
    "                            'metadata': {\n",
    "                                'chapter': chapter['title'],\n",
    "                                'start_page': chapter['start_page'],\n",
    "                                'end_page': chapter['end_page'],\n",
    "                                'chunk_type': 'content'\n",
    "                            }\n",
    "                        })\n",
    "                    current_chunk = sentence + '. '\n",
    "                    word_count = sentence_words\n",
    "                else:\n",
    "                    current_chunk += sentence + '. '\n",
    "                    word_count += sentence_words\n",
    "            if current_chunk.strip():\n",
    "                rag_chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'metadata': {\n",
    "                        'chapter': chapter['title'],\n",
    "                        'start_page': chapter['start_page'],\n",
    "                        'end_page': chapter['end_page'],\n",
    "                        'chunk_type': 'content'\n",
    "                    }\n",
    "                })\n",
    "        return rag_chunks\n",
    "\n",
    "    def retrieve_context(self, query, chapter_filter=None):\n",
    "        if not self.collection:\n",
    "            return {\"documents\": [[]], \"metadatas\": [[]]}\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        where_clause = None\n",
    "        if chapter_filter:\n",
    "            where_clause = {\"chapter\": {\"$eq\": chapter_filter}}\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=3,\n",
    "            where=where_clause\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def teach_topic(self, user_question, messages_history, selected_chapter=None, thread_id=\"default\"):\n",
    "        context_data = self.retrieve_context(user_question, selected_chapter)\n",
    "        context_text = \"\"\n",
    "        source_info = []\n",
    "        for i, doc in enumerate(context_data['documents'][0]):\n",
    "            if i < len(context_data['metadatas'][0]):\n",
    "                metadata = context_data['metadatas'][0][i]\n",
    "                context_text += f\"\\n--- Context {i+1} ---\\n{doc}\\n\"\n",
    "                source_info.append(f\"Chapter: {metadata['chapter']}, Pages: {metadata['start_page']}-{metadata['end_page']}\")\n",
    "        state = {\n",
    "            \"messages\": messages_history + [HumanMessage(content=user_question)],\n",
    "            \"context\": context_text,\n",
    "            \"sources\": source_info\n",
    "        }\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        output = self.app.invoke(state, config)\n",
    "        ai_response = output[\"messages\"][-1]\n",
    "        return {\n",
    "            \"response\": ai_response,\n",
    "            \"sources\": source_info,\n",
    "            \"context_used\": len(context_data['documents'][0])\n",
    "        }\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.document = None\n",
    "        self.filename = os.path.basename(file_path)\n",
    "        self.file_extension = os.path.splitext(self.filename)[1].lower()\n",
    "        self.page_char_counts = []\n",
    "\n",
    "    def open_document(self) -> bool:\n",
    "        try:\n",
    "            if self.file_extension == \".pdf\":\n",
    "                self.document = fitz.open(self.file_path)\n",
    "                return True\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {self.file_extension}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening {self.filename}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def close_document(self) -> None:\n",
    "        if self.document:\n",
    "            self.document.close()\n",
    "            self.document = None\n",
    "\n",
    "    def extract_text_fitz(self) -> str:\n",
    "        full_text = \"\"\n",
    "        self.page_char_counts = []\n",
    "        try:\n",
    "            if not self.document:\n",
    "                raise ValueError(\"Document not open.\")\n",
    "            for page in self.document:\n",
    "                page_text = page.get_text()\n",
    "                full_text += page_text\n",
    "                self.page_char_counts.append(len(page_text))\n",
    "            return full_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {self.filename}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_bookmarks_fitz(self) -> List[Tuple[str, int, int]]:\n",
    "        try:\n",
    "            if not self.document:\n",
    "                raise ValueError(\"Document not open.\")\n",
    "            toc = self.document.get_toc()\n",
    "            if not toc:\n",
    "                return []\n",
    "            bookmarks = []\n",
    "            for level, title, page in toc:\n",
    "                page_idx = page - 1 if page > 0 else 0\n",
    "                bookmarks.append((title, level, page_idx))\n",
    "            return bookmarks\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bookmarks from {self.filename}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def identify_chapters_regex(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        chapter_starts = []\n",
    "        patterns = [\n",
    "            (1, r\"(?:^|\\n)\\s*Chapter\\s+(\\d+)(?:\\s*[:.-]\\s*|\\s+)([^\\n]+)?\"),\n",
    "            (1, r\"(?:^|\\n)\\s*Part\\s+(\\d+|[IVX]+)(?:\\s*[:.-]\\s*|\\s+)([^\\n]+)?\"),\n",
    "            (1, r\"(?:^|\\n)\\s*Section\\s+(\\d+)(?:\\s*[:.-]\\s*|\\s+)([^\\n]+)?\"),\n",
    "            (1, r\"(?:^|\\n)\\s*\\d+\\.\\s+([A-Z][^\\n]+)\"),\n",
    "            (2, r\"(?:^|\\n)\\s*(\\d+\\.\\d+)(?:\\s*[:.-]\\s*|\\s+)([^\\n]+)?\"),\n",
    "            (3, r\"(?:^|\\n)\\s*(\\d+\\.\\d+\\.\\d+)(?:\\s*[:.-]\\s*|\\s+)([^\\n]+)?\"),\n",
    "            (2, r\"(?:^|\\n)\\s*([A-Z]\\.)\\s+([^\\n]+)\"),\n",
    "            (3, r\"(?:^|\\n)\\s*([a-z]\\.)\\s+([^\\n]+)\")\n",
    "        ]\n",
    "        for level, pattern in patterns:\n",
    "            for match in re.finditer(pattern, text):\n",
    "                title = match.group(0).strip()\n",
    "                chapter_starts.append((title, level, match.start()))\n",
    "        chapter_starts.sort(key=lambda x: x[2])\n",
    "        return chapter_starts\n",
    "\n",
    "    def find_page_number(self, char_index: int) -> int:\n",
    "        current_index = 0\n",
    "        for page_num, char_count in enumerate(self.page_char_counts):\n",
    "            if current_index <= char_index < current_index + char_count:\n",
    "                return page_num\n",
    "            current_index += char_count\n",
    "        return self.document.page_count - 1\n",
    "\n",
    "    def process_bookmark_chunks(self, bookmarks: List[Tuple[str, int, int]]) -> List[Dict[str, Any]]:\n",
    "        chunks = []\n",
    "        sorted_bookmarks = sorted(bookmarks, key=lambda x: x[2])\n",
    "        for i, (title, level, start_page) in enumerate(sorted_bookmarks):\n",
    "            end_page = self.document.page_count - 1\n",
    "            for j in range(i+1, len(sorted_bookmarks)):\n",
    "                next_title, next_level, next_page = sorted_bookmarks[j]\n",
    "                if next_level <= level:\n",
    "                    end_page = next_page - 1\n",
    "                    break\n",
    "            content = \"\"\n",
    "            for pg in range(start_page, min(end_page + 1, self.document.page_count)):\n",
    "                content += self.document.load_page(pg).get_text() + \"\\n\"\n",
    "            chunks.append({\n",
    "                \"title\": title,\n",
    "                \"level\": level,\n",
    "                \"start_page\": start_page,\n",
    "                \"end_page\": end_page,\n",
    "                \"content\": content.strip()\n",
    "            })\n",
    "        return chunks\n",
    "\n",
    "    def process_regex_chunks(self, chapter_starts: List[Tuple[str, int, int]], full_text: str) -> List[Dict[str, Any]]:\n",
    "        chunks = []\n",
    "        for i, (title, level, start_pos) in enumerate(chapter_starts):\n",
    "            start_page = self.find_page_number(start_pos)\n",
    "            if i + 1 < len(chapter_starts):\n",
    "                end_pos = chapter_starts[i + 1][2]\n",
    "                end_page = self.find_page_number(end_pos - 1)\n",
    "            else:\n",
    "                end_pos = len(full_text)\n",
    "                end_page = self.document.page_count - 1\n",
    "            content = full_text[start_pos:end_pos].strip()\n",
    "            chunks.append({\n",
    "                \"title\": title,\n",
    "                \"level\": level,\n",
    "                \"start_page\": start_page,\n",
    "                \"end_page\": end_page,\n",
    "                \"content\": content\n",
    "            })\n",
    "        return chunks\n",
    "\n",
    "    def build_hierarchical_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        chunks.sort(key=lambda x: (x[\"start_page\"], x[\"level\"]))\n",
    "        hierarchical_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"id\"] = i\n",
    "            chunk_copy[\"children\"] = []\n",
    "            hierarchical_chunks.append(chunk_copy)\n",
    "        for i, chunk in enumerate(hierarchical_chunks):\n",
    "            parent_id = None\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if hierarchical_chunks[j][\"level\"] < chunk[\"level\"]:\n",
    "                    parent_id = hierarchical_chunks[j][\"id\"]\n",
    "                    hierarchical_chunks[j][\"children\"].append(i)\n",
    "                    break\n",
    "            chunk[\"parent_id\"] = parent_id\n",
    "        return hierarchical_chunks\n",
    "\n",
    "    def process_pdf(self) -> Optional[Dict[str, Any]]:\n",
    "        if not self.open_document():\n",
    "            return None\n",
    "        try:\n",
    "            full_text = self.extract_text_fitz()\n",
    "            if not full_text:\n",
    "                print(f\"Could not extract text from {self.filename}.\")\n",
    "                return None\n",
    "            bookmarks = self.extract_bookmarks_fitz()\n",
    "            if bookmarks:\n",
    "                print(f\"Bookmarks found in {self.filename}. Using bookmarks for chapter identification.\")\n",
    "                chunks = self.process_bookmark_chunks(bookmarks)\n",
    "            else:\n",
    "                print(f\"No bookmarks found in {self.filename}. Using pattern detection for chapter identification.\")\n",
    "                chapter_starts = self.identify_chapters_regex(full_text)\n",
    "                chunks = self.process_regex_chunks(chapter_starts, full_text)\n",
    "            hierarchical_chunks = self.build_hierarchical_chunks(chunks)\n",
    "            return {\n",
    "                \"filename\": self.filename,\n",
    "                \"chunks\": chunks,\n",
    "                \"hierarchical_chunks\": hierarchical_chunks,\n",
    "                \"bookmarks\": bookmarks,\n",
    "                \"page_count\": self.document.page_count\n",
    "            }\n",
    "        finally:\n",
    "            self.close_document()\n",
    "\n",
    "def export_chunks_to_csv(chunks, filename):\n",
    "    df = pd.DataFrame(chunks)\n",
    "    export_df = df[['title', 'level', 'start_page', 'end_page']]\n",
    "    export_df['start_page'] += 1\n",
    "    export_df['end_page'] += 1\n",
    "    export_df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Exported structure to {filename}\")\n",
    "\n",
    "def print_hierarchical_toc(hierarchical_chunks):\n",
    "    sorted_chunks = sorted(hierarchical_chunks, key=lambda x: (x[\"start_page\"], x[\"level\"]))\n",
    "    for chunk in sorted_chunks:\n",
    "        display_level = 0\n",
    "        parent_id = chunk[\"parent_id\"]\n",
    "        while parent_id is not None:\n",
    "            display_level += 1\n",
    "            parent = next((c for c in hierarchical_chunks if c[\"id\"] == parent_id), None)\n",
    "            if parent:\n",
    "                parent_id = parent[\"parent_id\"]\n",
    "            else:\n",
    "                parent_id = None\n",
    "        indent = \"    \" * display_level\n",
    "        print(f\"{indent}- {chunk['title']} (Pages {chunk['start_page']+1}-{chunk['end_page']+1})\")\n",
    "\n",
    "# Notebook-friendly functions (replacing command line functionality)\n",
    "def process_book(pdf_path):\n",
    "    processor = PDFProcessor(pdf_path)\n",
    "    result = processor.process_pdf()\n",
    "    if not result:\n",
    "        print(\"Processing failed.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nProcessed: {result['filename']} ({result['page_count']} pages)\")\n",
    "    print(\"\\nTable of Contents:\")\n",
    "    print_hierarchical_toc(result[\"hierarchical_chunks\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def setup_teaching_assistant(book_result, api_key, model_name=\"llama-3.3-70b-versatile\"):\n",
    "    if not RAG_AVAILABLE:\n",
    "        print(\"RAG features are not available. Please install required packages:\")\n",
    "        print(\"pip install langchain-groq langgraph sentence-transformers chromadb\")\n",
    "        return None\n",
    "    \n",
    "    rag_system = BookTeachingRAG()\n",
    "    rag_system.index_book_content(book_result['chunks'])\n",
    "    rag_system.setup_groq_model(api_key, model_name)\n",
    "    \n",
    "    print(f\"Teaching assistant set up with model: {model_name}\")\n",
    "    return rag_system\n",
    "\n",
    "def ask_question(rag_system, question, chapter_filter=None, thread_id=\"notebook_session\"):\n",
    "    lc_messages = []\n",
    "    response = rag_system.teach_topic(\n",
    "        question,\n",
    "        lc_messages,\n",
    "        chapter_filter,\n",
    "        thread_id=thread_id\n",
    "    )\n",
    "    \n",
    "    print(\"AI Teacher Response:\\n\")\n",
    "    print(response[\"response\"].content)\n",
    "    if response[\"sources\"]:\n",
    "        print(\"\\nSources:\")\n",
    "        for source in response[\"sources\"]:\n",
    "            print(f\"- {source}\")\n",
    "            \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bada39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
